args:
  # General training config 
  train: True  # Whether to train the model 
  experiment_name: try_experiment  
  task: perturbation_modelling # task (cell_generation or perturbation_modelling)
  data_path: "/home/icb/alessandro.palma/environment/celldreamer/datasets/sciplex_complete_middle_subset.h5ad"
  batch_size: 512  
  resume: False
  train_autoencoder: False # Train autoencoder or only the diffusion model
  use_latent_repr: False # Latent diffusion or direct data feature diffusion 
  pretrained_autoencoder: False
  checkpoint_autoencoder: Null  
  pretrained_generative: False
  chekpoint_path: Null  
  
  # Perturbation setting specific
  perturbation_key: condition 
  dose_key: dose
  covariate_keys: cell_type
  smile_keys: SMILES
  degs_key: lincs_DEGs
  pert_category: cov_drug_dose_name
  split_key: split_ho_pathway
  use_drugs: True
  feature_type: grover
  freeze_embeddings: True  # Whether the embeddings should get trained 
  use_drugs: True  # Use drug or not 
  doser_width: 128
  doser_depth: 3  
  embedding_dimensions: 100  # Dimensionality of t
  one_hot_encode_features: False
             
  # General model 
  generative_model: diffusion  
  denoising_model: mlp
                    
  # Autoencoder 
  autoencoder_kwargs: Null

  # Checkpoint kwargs 
  checkpoint_kwargs: 
    filename: epoch_{epoch:01d}
    monitor: loss/valid_loss
    mode: min
    save_last": True
    auto_insert_metric_name: False

  # Early stopping kwargs 
  early_stopping_kwargs: 
    monitor: loss/valid_loss
    patience: 20
    mode: min
    min_delta: 0.
    verbose: False
    strict: True
    check_finite: True
    stopping_threshold: None
    divergence_threshold: Null
    check_on_train_epoch_end: Null

  # Logger kwargs
  logger_kwargs: 
    offline: False
    id: Null 
    anonymous": Null 
    project: celldreamer
    log_model: False 
    prefix: "" 
    tags: []
    job_typex: ""   

  #Denoising model specific 
  denoising_module_kwargs: 
    dims: [128, 64]
    time_embed_size: 100 
    class_emb_size: 100
    dropout: 0.0
                  
  # Diffusion model specific
  generative_model_kwargs: 
    T: 4000
    w: 0.3
    v: 0.2
    p_uncond: 0.2
    logging_freq: 1000
    classifier_free: False
    learning_rate: 0.001
    weight_decay: 0.0001
                  
  # Autoencoder trainer hparams
  trainer_autoencoder_kwargs:
    max_epochs: 1000
    gradient_clip_val: 1.
    gradient_clip_algorithm: norm
    accelerator: gpu
    devices: 1
    check_val_every_n_epoch: 1
    log_every_n_steps: 100
    detect_anomaly: False
    deterministic: False
                      
  # Generative model trainer hyperparams 
  trainer_generative_kwargs:
    max_epochs: 1000
    gradient_clip_val: 1
    gradient_clip_algorithm: norm
    accelerator: gpu
    devices: 1
    num_sanity_val_step: 0
    check_val_every_n_epoch: 1
    log_every_n_steps: 100
    detect_anomaly: False 
    deterministic: False            