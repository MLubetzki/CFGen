x0_from_x_kwargs:
  dims: [512, 512]
  batch_norm:  True
  dropout: False 
  dropout_p: 0.0
learning_rate: 0.0001 
weight_decay: 0.01 
antithetic_time_sampling: True 
scaling_method: log_normalization
pretrain_encoder: True
pretraining_encoder_epochs: 1
sigma: 0.3