x0_from_x_kwargs:
  dims: [512, 512]
  batch_norm: True
  dropout: False 
  dropout_p: 0.0
learning_rate: 0.0001 
weight_decay: 0.00001 
antithetic_time_sampling: True 
scaling_method: log_normalization
pretrain_encoder: True
pretraining_encoder_epochs: 200
sigma: 0.0001
covariate_specific_theta: False
plot_and_eval_every: 100